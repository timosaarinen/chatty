```
╭───────────────────────── Chatty - Local Code Agent ─────────────────────────╮
│                                                                             │
│              ██████╗██╗  ██╗ █████╗ ████████╗████████╗██╗   ██╗             │
│             ██╔════╝██║  ██║██╔══██╗╚══██╔══╝╚══██╔══╝╚██╗ ██╔╝             │
│              ██║     ███████║███████║   ██║      ██║    ╚████╔╝             │
│              ██║     ██╔══██║██╔══██║   ██║      ██║     ╚██╔╝              │
│               ╚██████╗██║  ██║██║  ██║   ██║      ██║      ██║              │
│                ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝   ╚═╝      ╚═╝      ╚═╝              │
│                                                                             │
╰────────── A Local-First, Code-Executing AI Agent Powered by Ollama ─────────╯
```

`chatty` is a minimalist, local-first terminal AI assistant. It connects to local [Ollama](https://ollama.com/) models and is designed for developers who want an agent that can execute Python code to solve problems, access local files, and interact with external services.

## How It Works

The core concept is to empower an LLM to write and execute Python code. `chatty` uses `uv run` to execute this code, which creates a temporary virtual environment for dependency management. For true security and filesystem isolation, **running `chatty` inside the provided Docker container is the recommended and safest method.**

The agent provides a special `Tools` class that can be used within the generated code. This class acts as a proxy, forwarding tool calls to either:
1.  **Internal Tools:** Built-in Python functions defined within the agent's code.
2.  **MCP Servers:** External, hot-pluggable [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/modelcontextprotocol) servers that expose their own tools. This allows `chatty`'s capabilities to be extended dynamically without modifying its core logic.

## Requirements

*   [**Docker**](https://www.docker.com/): For running the agent in a secure, sandboxed environment.
*   [**Ollama**](https://ollama.com/): Must be installed and running on your host machine with at least one model downloaded (e.g., `ollama pull codellama`).
*   **User Discretion**: `chatty` executes Python code generated by an LLM. **You are responsible for reviewing and approving the code before it runs.** Even within Docker, you should understand the code's intent.

## Installation & Usage (Docker Recommended)

Running `chatty` inside a Docker container is the safest way to use the agent. It provides a strong security sandbox for code execution and simplifies dependency management for external MCP servers.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/timosaarinen/chatty.git
    cd chatty
    ```
2.  **Build the Docker image:**
    The provided script handles building the `chatty` image.
    ```bash
    scripts/build.sh
    ```

### Running the Agent

Use the `run.sh` script to start an interactive session. You must provide a model name. The script ensures the container can connect to your host's Ollama instance.

```bash
# Run with a specific model
scripts/run.sh codellama:latest

# Pass additional arguments to chatty.py
scripts/run.sh qwen2.5-coder:7b --mcp mcp-config/demo-only.json --debug
```

### Development Workflow

For active development, use `dev.sh` to start a long-running container with your local project directory mounted.

1.  **Start the development container:**
    ```bash
    scripts/dev.sh
    ```
2.  **Get a shell inside the container:**
    ```bash
    scripts/exec.sh
    ```
3.  **Run `chatty` from inside the container shell:**
    From this shell, you can run the agent as you would locally.
    ```bash
    # Inside the container from exec.sh
    uv run chatty.py --model <your-model>
    ```

## Advanced: Local Usage (Without Docker)

You can run `chatty` directly on your host if you have `uv` installed.

**⚠️ Security Warning**: This method does not provide a security sandbox. The Python code generated by the LLM will be executed directly on your machine with the same permissions as your user account. This is **not recommended** unless you are developing the agent itself and understand the risks.

### Requirements

*   [**uv**](https://github.com/astral-sh/uv): For running the script and managing dependencies.
*   [**Ollama**](https://ollama.com/): Must be installed and running.

### Usage

You must specify an Ollama model to use. If you are unsure which models you have, run `ollama list`.

```bash
# Example using a common coding model
uv run chatty.py --model codellama:latest

# Example with a different model and MCP configuration
uv run chatty.py --model qwen2.5-coder:7b --mcp mcp-config/demo-and-fetch.json
```
  