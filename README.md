# chatty - Experimental Code Agent with Local Ollama LLM and MCP support

```bash
uv run chatty.py --model qwen2.5-coder:7b --mcp mcp-config/demo-and-fetch.json
```

Replace the model with any local Ollama model you have installed (without --model lists the models you have, if Ollama server is running).

## Requirements

- uv
- Ollama
