```
╭───────────────────────── Chatty - Local Code Agent ─────────────────────────╮
│                                                                             │
│              ██████╗██╗  ██╗ █████╗ ████████╗████████╗██╗   ██╗             │
│             ██╔════╝██║  ██║██╔══██╗╚══██╔══╝╚══██╔══╝╚██╗ ██╔╝             │
│              ██║     ███████║███████║   ██║      ██║    ╚████╔╝             │
│              ██║     ██╔══██║██╔══██║   ██║      ██║     ╚██╔╝              │
│               ╚██████╗██║  ██║██║  ██║   ██║      ██║      ██║              │
│                ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝   ╚═╝      ╚═╝      ╚═╝              │
│                                                                             │
╰────────── A Local-First, Code-Executing AI Agent Powered by Ollama ─────────╯
```

`chatty` is a minimalist, local-first terminal AI assistant. It connects to local [Ollama](https://ollama.com/) models and is designed for developers who want an agent that can execute Python code to solve problems, access local files, and interact with external services.

## How It Works

The core concept is to empower an LLM to write and execute Python code in a sandboxed environment using `uv run`. `chatty` provides a special `Tools` class that can be used within the generated code. This class acts as a proxy, forwarding tool calls to either:
1.  **Internal Tools:** Built-in Python functions defined within the agent's code.
2.  **MCP Servers:** External, hot-pluggable [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/modelcontextprotocol) servers that expose their own tools *with latest "MCP v2" or "2025-03-26" protocolVersion.*

This architecture allows the agent's capabilities to be extended dynamically without modifying its core logic.

## Requirements

*   [**uv**](https://github.com/astral-sh/uv): For running the script and managing dependencies.
*   [**Ollama**](https://ollama.com/): Must be installed and running with at least one model downloaded.
*   **Python Knowledge & User Discretion**: `chatty` executes Python code generated by an LLM. While it runs scripts in an isolated environment, **you are responsible for reviewing and approving the code before it runs.** You should have a basic understanding of Python to assess the safety and intent of the proposed scripts.

## Installation & Usage

Currently, `chatty` must be run from a local clone of the repository.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/timosaarinen/chatty.git
    cd chatty
    ```

2.  **Run the agent:**
    You must specify an Ollama model to use. If you are unsure which models you have, run `ollama list`.

    ```bash
    # Example using a common coding model
    uv run chatty.py --model codellama:latest

    # Example with a different model and MCP configuration
    uv run chatty.py --model qwen2.5-coder:7b --mcp mcp-config/demo-and-fetch.json
    ```
  