```
╭───────────────────────── Chatty - Local Code Agent ─────────────────────────╮
│                                                                             │
│              ██████╗██╗  ██╗ █████╗ ████████╗████████╗██╗   ██╗             │
│             ██╔════╝██║  ██║██╔══██╗╚══██╔══╝╚══██╔══╝╚██╗ ██╔╝             │
│              ██║     ███████║███████║   ██║      ██║    ╚████╔╝             │
│              ██║     ██╔══██║██╔══██║   ██║      ██║     ╚██╔╝              │
│               ╚██████╗██║  ██║██║  ██║   ██║      ██║      ██║              │
│                ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝   ╚═╝      ╚═╝      ╚═╝              │
│                                                                             │
╰────────── A Local-First, Code-Executing AI Agent Powered by Ollama ─────────╯
```

`chatty` is a minimalist, local-first terminal AI assistant for developers. It runs code in your environment, connects to [Ollama](https://ollama.com/) models, and can access tools exposed via the Model Context Protocol (MCP).


## Features

- **Local-first:** No cloud by default; everything runs on your machine.
- **Code execution:** Lets an LLM write and run Python code (in a sandbox if Docker is used).
- **Tooling:** Accesses both built-in functions and external MCP tools.
- **Extensible:** Plug in more tools at runtime via MCP.
- **Hot-Reloading:** MCP tools and prompts can be reloaded at runtime.

## Requirements

- [**Docker**](https://www.docker.com/): Recommended for sandboxed, secure execution.
- [**Ollama**](https://ollama.com/): Must be running with at least one model installed.

> **Docker is strongly recommended for security and easy setup.**

`chatty` executes Python code generated by an LLM. While the recommended Docker setup provides a strong sandbox, be aware that the agent can interact with external services and local files within its container.

## How It Works

The core concept is to empower an LLM to write and execute Python code. `chatty` uses `uv run` to execute this code, which creates a temporary virtual environment for dependency management. For true security and filesystem isolation, **running `chatty` inside the provided Docker container is the recommended and safest method.**

The agent provides a special `Tools` class that can be used within the generated code. This class acts as a proxy, forwarding tool calls to either:
1.  **Internal Tools:** Built-in Python functions defined within the agent's code.
2.  **MCP Servers:** External, hot-pluggable [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/modelcontextprotocol) servers that expose their own tools. This allows `chatty`'s capabilities to be extended dynamically without modifying its core logic.

## Installation & Usage (Docker Recommended)

Running `chatty` inside a Docker container is the safest way to use the agent. It provides a strong security sandbox for code execution and MCP servers.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/timosaarinen/chatty.git
    cd chatty
    ```
2.  **Build the Docker image:**
    The provided script handles building the `chatty` image.
    ```bash
    scripts/build.sh
    ```

### Running the Agent

Use the `run.sh` script to start an interactive session. You must provide a model name. The script ensures the container can connect to your host's Ollama instance.

```bash
# Run with a specific model
scripts/run.sh codellama:latest

# Pass additional arguments to chatty.py
scripts/run.sh qwen2.5-coder:7b --mcp mcp-config/demo-only.json --debug
```

### Development Workflow

For active development, use `dev.sh` to start a long-running container with your local project directory mounted.

1.  **Start the development container:**
    ```bash
    scripts/dev.sh
    ```
2.  **Get a shell inside the container:**
    ```bash
    scripts/exec.sh
    ```
3.  **Run `chatty` from inside the container shell:**
    From this shell, you can run the agent as you would locally.
    ```bash
    # Inside the container from exec.sh
    uv run chatty.py --model <your-model>
    ```

## Local Usage (Without Docker)

You can run `chatty` directly on your host if you have `uv` installed.

**⚠️ Security Warning**: This method does not provide a security sandbox. MCP tool calls and the Python code generated by the LLM will be executed directly on your machine with the same permissions as your user account. This is **not recommended** unless you understand the risks.

### Requirements

*   [**uv**](https://github.com/astral-sh/uv): For running the script and managing dependencies.
*   [**Ollama**](https://ollama.com/): Must be installed and running.

### Usage

You must specify an Ollama model to use. If you are unsure which models you have, run `ollama list`.

```bash
# Example using a common coding model
uv run chatty.py --model codellama:latest

# Example with a different model and MCP configuration
uv run chatty.py --model qwen2.5-coder:7b --mcp mcp-config/demo-and-fetch.json
```
